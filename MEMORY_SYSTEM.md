# Memory System Documentation

## Overview

The Mini AI Tutor implements an industry-level conversational memory system that enables the AI to remember user information, preferences, and past interactions across conversations. The memory architecture uses a multi-tiered approach combining short-term verbatim recall, working memory with intelligent summarization, and long-term semantic storage. The system achieves 60-80% token reduction through smart compression while maintaining conversation continuity and personalization. Storage spans Redis for caching, MongoDB for persistent memories, and ChromaDB for semantic vector search.

The memory implementation follows ten advanced strategies from production-grade conversational AI systems including multi-tiered memory, semantic retrieval with multi-factor relevance scoring, memory consolidation pipelines, hierarchical organization with namespaces, intelligent forgetting with decay, contextual memory injection, distributed multi-user architecture, meta-memory reflection, privacy compliance, and advanced context window budget management. Background jobs handle periodic consolidation, decay application, cleanup, and health monitoring ensuring the system scales efficiently to millions of users.

## Multi-Tiered Memory Architecture

The memory system organizes information across three distinct tiers optimized for different retrieval patterns and retention periods. Short-term memory maintains the last 5 messages verbatim providing immediate conversation context with a 5-minute TTL in Redis. Working memory covers the current session with up to 20 messages, applying summarization when conversations exceed 10 messages to reduce token usage while preserving key information with a 2-hour TTL. Long-term memory stores consolidated facts, preferences, experiences, skills, goals, and relationships extracted from past conversations in MongoDB with semantic embeddings in ChromaDB for relevance-based retrieval.

The tiered approach allocates context window budget efficiently with 25% for system prompts, 20% for short-term memory, 20% for working memory, 20% for long-term memory, 10% for the current message, and 5% buffer. The total context budget defaults to 2000 tokens ensuring responses stay within model limits while maximizing relevant information. Each tier serves a specific purpose with short-term providing exact recent context, working memory bridging immediate and historical information through summarization, and long-term offering semantic recall of user characteristics discovered over time.

The ConversationManager in backend/ai/memory/conversationManager.js handles short-term and working memory tiers using Redis caching for fast context retrieval. Sessions are identified by userId and conversationId combinations with automatic TTL expiration preventing memory bloat. When conversations exceed the verbatim threshold, older messages are summarized using the LLM while recent messages remain unchanged ensuring the assistant has both historical context and precise recent exchanges. Cache hit rates exceed 75% under typical usage patterns dramatically reducing database queries.

The IndustryMemoryManager in backend/ai/memory/industryMemoryManager.js orchestrates all three tiers providing a unified getMultiTieredMemory interface that retrieves appropriate context for each conversation turn. The system checks Redis cache first for sub-millisecond retrieval, fetches recent messages from MongoDB for short-term context, generates or retrieves session summaries for working memory, performs semantic search in ChromaDB for relevant long-term memories, and enriches context with user profile information. The complete process typically completes in under 100ms enabling real-time conversational experiences.

## Semantic Memory Retrieval and Relevance Scoring

Long-term memory retrieval uses semantic search combining vector similarity with multi-factor relevance scoring to surface the most useful memories for each conversation. When a user sends a message, the system generates an embedding using the BGE-small model and queries the user-specific ChromaDB collection for semantically similar memories. The vector search uses HNSW indexing with M=16 graph connections, efConstruct=200 for index building, and efSearch=50 for query-time accuracy returning the top 10 candidate memories with minimum 0.3 similarity scores.

Candidate memories undergo multi-factor relevance scoring combining five weighted factors to produce final rankings. Semantic similarity from vector search contributes 30% measuring how closely the memory content matches the current query. Recency contributes 25% using exponential decay with a 14-day half-life calculated as exp(-0.05 * ageInDays) so newer memories rank higher. Frequency contributes 20% based on access count using log normalization to prevent extremely popular memories from dominating. Importance contributes 15% from pre-calculated memory importance scores. Emotional valence contributes 10% using absolute values since strong emotions whether positive or negative tend to be more memorable and relevant.

The relevance scoring formula produces values from 0 to 1 with higher scores indicating greater utility for the current conversation. An optional intent matching bonus adds 0.2 to the score when the memory's topic aligns with conversation intent detected through semantic classification. After scoring, memories are sorted by relevance score and the top 5 are selected for injection into context. Selected memories are marked as accessed updating lastAccessedAt timestamps and incrementing access frequency counters which feeds back into future relevance calculations creating a virtuous cycle where useful memories become more retrievable.

The semantic retrieval implementation in backend/ai/memory/industryMemoryManager.js method getLongTermMemory handles embedding generation, vector search, MongoDB fetching, relevance calculation, sorting, selection, and access tracking. Error handling ensures graceful degradation when ChromaDB is unavailable falling back to empty long-term memory rather than failing the entire conversation. The system logs retrieval metrics enabling monitoring of memory system effectiveness and identifying opportunities for tuning relevance weights.

## Memory Storage Schemas

Memories are stored using two primary MongoDB schemas: MemoryEntry for individual memories and UserProfile for consolidated user characteristics. The MemoryEntry schema in backend/models/MemoryEntry.js defines a comprehensive structure capturing memory content, type classification, namespace organization, entities, temporal information, importance scoring, source provenance, semantic metadata, versioning, lifecycle status, privacy settings, access control, and audit trails.

Memory content is limited to 5000 characters with rich metadata enabling sophisticated retrieval and management. Memory types include fact for objective information, preference for user likes and dislikes, experience for past events, skill for abilities, goal for learning objectives, relationship for people connections, and event for significant occurrences. The hierarchical namespace organizes memories by category (personal, work, education, hobby, health, general), optional subcategory, and optional topic enabling filtering and browsing beyond pure semantic search.

Temporal information tracks creation timestamp, last update timestamp, last access timestamp for recency calculations, optional expiration timestamp for automatic forgetting, and time context strings like "2024-01" or "last week" for temporal reasoning. Importance scoring combines multiple factors including userMarked boolean for explicit flagging, accessFrequency counter incremented on each retrieval, recency decay factor from exponential formula, emotionalValence from -1 to 1, and contradictionCount tracking how often the memory conflicts with new information. The composite importance score from 0 to 1 is calculated using weighted combination of these factors.

Source provenance tracks where memories originated including conversationId reference, messageIds array for exact message links, extractionMethod indicating automatic extraction versus user explicit statements versus consolidation versus inference, and confidence score from 0 to 1 reflecting extraction certainty. Semantic metadata includes embeddingId pointing to the ChromaDB vector, keywords array for term-based search, relatedMemoryIds linking connected memories, and similarityThreshold for relationship detection. Version history maintains evolution of memories through corrections, refinements, and consolidations with full historical record.

The UserProfile schema in backend/models/UserProfile.js consolidates extracted information into a rich user model covering personal details (name, nickname, role, location, languages), professional information (occupation, industry, skills with proficiency levels, current role, company, experience), learning profile (goals with priority and status, interests with strength and expertise ratings, learning style preferences including format/pace/depth, current course enrollments), preferences (communication formality/length/tone, interaction style, topic favorites and avoidance, timing and active hours), behavioral patterns (engagement metrics, usage patterns, satisfaction scores), relationships (mentioned people and organizations with context), and psychometric profiles (personality traits, emotional patterns, cognitive style).

Profile completeness is calculated across five dimensions with weighted scoring: personal information 20%, professional 15%, learning profile 25%, preferences 20%, and behavioral patterns 20%. The system tracks completeness percentage enabling progressive enhancement as more information is discovered. Metadata tracks total memories, memory type distribution, data quality metrics including average confidence and contradictions, and privacy settings controlling analytics consent and data retention policies. Methods on the schema calculate completeness, update engagement metrics from sessions, and manage interests with strength adjustments.

## Memory Consolidation Pipeline

Memory consolidation transforms ephemeral conversation messages into durable long-term memories through automated extraction, deduplication, storage, and profile updates. Background jobs in backend/ai/memory/memoryJobs.js run consolidation daily finding conversations older than 24 hours not yet consolidated and processing them in batches of 10. The consolidation process analyzes conversation messages extracting explicit facts, preferences, goals, experiences, and entities using pattern matching and natural language processing techniques.

The extraction phase in the consolidateMemories method retrieves all messages from target conversations and applies regex patterns to identify common self-disclosure patterns. Name extraction matches phrases like "I'm John Smith" or "my name is Mary" with confidence 0.8. Occupation extraction matches "I work as a software engineer" or "I'm a student" storing roles with confidence 0.8. Preference extraction matches "I love Python" or "I prefer functional programming" capturing likes and interests. Goal extraction matches "I want to learn machine learning" or "I need to understand calculus" identifying learning objectives. Experience extraction matches "I'm learning React" or "I'm working on a project" tracking current activities.

Entity extraction identifies people, organizations, and other named entities from conversation text using pattern matching with plans to integrate proper NER libraries like spaCy or Stanford NER in production. Extracted entities include type classification (person, organization, location, skill), value text, and confidence scores. Entities are stored in the memory entry's entities array enabling entity-based retrieval and relationship analysis. The extraction produces a collection of candidate memories each with content, type, namespace, source metadata, and importance factors.

Deduplication prevents storing redundant or duplicate memories by comparing new extractions against existing memories using Jaccard similarity on word sets. When similarity exceeds 0.9 indicating a duplicate, the system merges rather than creating new entries. Merging increments the existing memory's access frequency, updates last accessed timestamp, appends new source message IDs, and adds a version history entry recording the consolidation event. This approach strengthens existing memories through reinforcement rather than fragmenting knowledge across duplicates.

Storage persists deduplicated memories in both MongoDB and ChromaDB atomically. The MemoryEntry document is created in MongoDB with all metadata and relationships. An embedding vector is generated from the memory content using the BGE-small model. The embedding is stored in a user-specific ChromaDB collection with metadata linking back to the MongoDB memory ID. The embeddingId field in the MemoryEntry is updated with the ChromaDB reference completing the bidirectional link. Profile updates extract patterns from consolidated memories updating UserProfile fields like name, role, interests, goals, and skills with confidence scores and timestamps.

## Intelligent Forgetting and Memory Decay

The memory system implements intelligent forgetting using decay functions and importance thresholds to archive low-value memories while preserving significant information indefinitely. Memory decay jobs run daily applying exponential decay to all active memories based on age and access patterns. The decay rate defaults to 0.1 meaning memories lose 10% importance per day unless reinforced through access. The recency factor is calculated as exp(-decayRate * daysSinceCreated) producing exponential decay curves where recent memories maintain high scores while old unaccessed memories decay toward zero.

The shouldForget method on MemoryEntry evaluates whether a memory should be archived based on multiple criteria. User-marked important memories indicated by the userMarked flag are never forgotten regardless of age or access patterns respecting explicit user intent. Automatic forgetting occurs when memories are older than 90 days with importance scores below 0.2 indicating low value information. Memories not accessed in the last 60 days are archived assuming they are no longer relevant to the user's current interests. Memories with explicit expiration timestamps that have passed are archived as originally intended. Forgotten memories transition to archived status remaining queryable if needed but excluded from normal retrieval.

The decay job processes users in batches of 50 preventing overwhelming the system during large-scale operations. For each user, all active memories are loaded and decay calculations are applied. The calculateImportanceScore method recalculates composite scores incorporating fresh recency values and access frequency updates. Memories meeting forgetting criteria are marked archived while others are updated with new importance scores. Statistics track forgotten and decayed memory counts enabling monitoring of forgetting rates and system health. High forgetting rates above 50% trigger warnings indicating aggressive decay settings or insufficient user engagement.

Decay configuration balances memory retention with performance and relevance. The current 14-day half-life through the 0.05 decay coefficient means memories retain approximately 50% importance after two weeks, 25% after four weeks, and 12.5% after six weeks unless reinforced. Access reinforcement counteracts decay by incrementing access frequency and refreshing last accessed timestamps. Frequently retrieved memories develop high frequency scores maintaining importance despite age. The weighted combination of recency, frequency, and other factors produces nuanced importance calculations where useful recurring information persists while one-time mentions fade naturally.

## Context Window Budget Management

The memory system manages limited context window space through intelligent budget allocation ensuring each context component receives appropriate token allocation. The context budget configuration in IndustryMemoryManager allocates the 2000-token default budget across system prompts (25%), short-term memory (20%), working memory (20%), long-term memory (20%), current message (10%), and buffer (5%). These percentages are converted to absolute token counts during context building with 500 tokens for system prompts, 400 tokens each for short-term, working, and long-term memory, 200 tokens for the current message, and 100 token buffer for safety.

Token estimation uses a simple approximation of 4 characters per token calculated as Math.ceil(text.length / 4) providing rough token counts without expensive tokenizer calls. While less accurate than actual tokenization, the estimate is fast and conservative erring on the side of slightly underestimating available space. The formatMemoriesForInjection method builds context strings respecting budget allocations by calculating running token totals and truncating when limits are reached. Each tier is formatted independently enabling partial inclusion when full content exceeds budgets.

Short-term memory formatting converts recent messages into "role: content" pairs maintaining chronological order. The formatter adds messages until the 400-token short-term budget is exhausted ensuring at least the most recent messages appear. Working memory formatting includes session summaries when available followed by recent messages not in short-term context. Summaries provide compressed historical context while recent verbatim messages ensure precision. Long-term memory formatting adds retrieved memories sorted by relevance score highest first until the 400-token long-term budget is consumed. Each memory includes content, type, and timestamp information.

User profile formatting extracts key profile fields into concise summary lines appearing at the context start. Profile lines include name, role, top skills, top interests, learning style preferences, and communication preferences when available. The profile section typically consumes 100-150 tokens providing essential personalization context. The complete formatted context combines profile, long-term memories, working memory summary, and short-term messages into a natural language narrative injected before the current message. This structure ensures the AI has comprehensive context within token constraints.

Context truncation handles budget overruns when allocated space is insufficient for all available information. The truncateContext method estimates total tokens and compares against the maximum allowed budget. When estimates exceed limits, the text is truncated to fit by slicing to maxTokens * 4 characters and appending a truncation indicator. Truncation logs warnings enabling monitoring of frequent budget violations suggesting the need for budget adjustments or more aggressive summarization. Token budget management ensures consistent performance across conversation lengths preventing context overflow errors while maximizing information density.

## Distributed Architecture and Caching

The memory system employs distributed architecture using Redis, MongoDB, and ChromaDB to achieve scalability, performance, and fault tolerance. Redis serves as the L1 cache layer storing session contexts, summaries, and frequently accessed data with automatic TTL expiration. MongoDB provides the persistent storage layer for memory entries, user profiles, conversations, and messages with rich querying and indexing. ChromaDB handles vector storage and semantic search with HNSW indexing for sub-second nearest neighbor retrieval. The three-tier storage enables horizontal scaling with independent resource allocation per layer.

Redis caching achieves 75-80% hit rates under normal usage dramatically reducing database load and query latency. Conversation contexts are cached with keys like "conversation:userId:conversationId" storing optimized context structures including recent messages, summaries, and metadata. Cache TTLs of 5 minutes for short-term, 2 hours for working memory, and 24 hours for summaries align with tier definitions ensuring fresh data while minimizing database round trips. Cache invalidation occurs automatically through TTL expiration and explicitly when new messages arrive or memories are consolidated.

Memory retrieval statistics tracked in the IndustryMemoryManager include cache hits, cache misses, total retrievals, consolidations, and forgetting events. Cache hit rate is calculated as cacheHits / (cacheHits + cacheMisses) with rates below 50% triggering warnings in health checks. Low hit rates indicate high query variance, frequent cache evictions, or insufficient cache size requiring investigation. Statistics are exposed through getStats method and monitored by background health check jobs running hourly.

MongoDB indexing optimizes memory queries with compound indexes on frequently accessed field combinations. The MemoryEntry collection has indexes on userId + namespace.category + temporal.createdAt for category browsing, userId + type + status for type filtering, userId + importance.score + temporal.lastAccessedAt for relevance queries, userId + source.conversationId for conversation-specific retrieval, temporal.expiresAt for TTL-based expiration, and semantic.embeddingId for ChromaDB linkage. These indexes enable sub-10ms query times for typical retrieval patterns supporting real-time conversation flows.

ChromaDB collections are created per user with naming pattern "user_memories_userId" enabling user-level sharding and access control. User-specific collections prevent cross-user data leakage, enable independent deletion for user data requests, support per-user quotas and rate limiting, and scale horizontally by distributing users across ChromaDB instances. Collection metadata includes memory IDs, types, categories, and timestamps enabling filtered semantic search. The chromaService abstraction in backend/ai/vectorstore/chromaService.js handles collection management, document addition, and search queries with error handling and retry logic.

## Background Maintenance Jobs

Background jobs ensure memory system health through periodic consolidation, decay application, cleanup, and monitoring implemented in backend/ai/memory/memoryJobs.js. The MemoryJobs class manages four distinct job types each running on independent schedules with configurable intervals and batch sizes. Jobs use setInterval for periodic execution with immediate initial runs ensuring maintenance begins at startup. Jobs can be manually triggered through the triggerJob method for testing or emergency maintenance.

The consolidation job runs daily finding conversations completed more than 24 hours ago without the consolidated flag set in metadata. The job queries Conversation collection with filters lastMessageAt < cutoffDate, isActive: true, and metadata.consolidated != true returning up to 10 conversations per batch. For each conversation, the consolidateMemories method extracts memories, deduplicates against existing entries, stores in MongoDB and ChromaDB, and updates user profiles. After successful consolidation, the conversation's metadata.consolidated flag is set true with metadata.consolidatedAt timestamp preventing reprocessing. Results track success count, failure count, and total memories consolidated.

The decay job runs daily processing up to 50 users per batch found through UserProfile collection queries. For each user, the applyMemoryDecay method loads all active memories, recalculates importance scores with updated recency factors, evaluates shouldForget criteria, and archives low-value memories. The job tracks total forgotten memories indicating effective decay and total decayed memories indicating importance updates without archiving. Decay results are logged enabling monitoring of forgetting rates and identifying users with unusual patterns.

The cleanup job runs weekly archiving memories not accessed in 90 days that lack user-marked importance flags. The job uses MongoDB updateMany with filters status: active, temporal.lastAccessedAt < cutoffDate, importance.factors.userMarked: false updating matched documents to status: archived. Expired memories with temporal.expiresAt < currentTime are permanently deleted using deleteMany preventing indefinite retention. Cleanup results track archived count and deleted count providing visibility into data lifecycle management. The 90-day retention balances keeping potentially relevant information against storage cost and privacy concerns.

The health check job runs hourly gathering memory system statistics and identifying potential issues. Statistics include total retrievals, consolidations, forgetting events, and cache hit rates. The job calculates derived metrics like cache hit percentage and forgetting rate percentage comparing against thresholds. Issues detected include low cache hit rate below 50% indicating cache ineffectiveness and high forgetting rate above 50% indicating aggressive decay. Issues are logged with severity levels (info, warning, error) enabling alerting and investigation. Health metrics support capacity planning, performance optimization, and reliability monitoring.

## Privacy, Compliance, and Security

The memory system implements privacy controls, compliance measures, and security safeguards protecting user data and respecting individual preferences. The MemoryEntry privacy schema defines four privacy levels: public for shareable information, private for personal but non-sensitive data (default), sensitive for health/financial information requiring extra protection, and confidential for highly restricted data never shared. Data categories classify memory content as general, personal, health, financial, biometric, or special with special categories subject to GDPR-style enhanced protections.

User consent management tracks whether users granted permission for memory storage with consent.granted boolean and consent.grantedAt timestamp. The system defaults to true for general data but requires explicit consent for sensitive categories. Consent can be withdrawn retroactively triggering deletion of affected memories through compliance workflows. Retention policies define data lifecycle rules with options for standard (365 days), extended (unlimited for educational records), minimal (90 days for temporary data), and explicit_consent (indefinite with active consent). Expired data is automatically deleted through cleanup jobs.

Access control limits who can read, modify, or delete each memory through readBy, modifyBy, and deleteBy arrays containing authorized principals. Default access allows the user and system components to read their own memories while restricting administrators and other users. Course instructors may receive limited read access to student learning memories when consent is granted. Modifications and deletions require explicit authorization preventing unauthorized tampering. Access control is enforced at the database query level with middleware checking permissions before returning results.

Audit trails track all significant memory operations including created, accessed, updated, consolidated, deprecated, and deleted actions. Each audit entry records the action type, timestamp, actor ID, and optional details object with additional context. Audit logs enable compliance investigations, security incident response, and user data access requests. The logs are append-only preventing tampering and retained longer than the underlying data per regulatory requirements. Privacy-preserving analytics aggregate audit data removing personally identifiable information for system monitoring.

The filterMemoriesForPrivacy method enables selective memory retrieval based on privacy settings and user preferences. Options include includeCategory array filtering memories to specific namespace categories, excludeCategory array blocking categories, and userConsentOnly boolean requiring explicit consent. Sensitive data categories are filtered unless user consent exists. Confidential memories are never included in standard retrieval protecting highly restricted information. Privacy filtering is applied before relevance scoring ensuring sensitive memories never leak through semantic search.

## Integration with AI Orchestration

The memory system integrates tightly with the AI orchestrator providing contextualized prompts for every conversation turn. The integration point in backend/ai/orchestrator/aiOrchestrator.js calls getMultiTieredMemory before generating AI responses passing userId, conversationId, and currentMessage. Retrieved memory structures are formatted using formatMemoriesForInjection producing natural language context injected into the system prompt. The formatted context provides the AI with comprehensive user knowledge enabling personalized, coherent responses referencing past interactions.

Memory-enhanced prompts follow a structured format starting with "About the user" section containing profile information, followed by "What you remember about the user" section with relevant long-term memories sorted by relevance, then "Earlier in this conversation" section with working memory summary, and finally "Recent messages" section with short-term verbatim context. This organization prioritizes enduring user characteristics over transient conversation details matching human memory retrieval patterns. The AI receives rich context without explicit instruction to reference memories relying on natural language priming.

Conversation continuity emerges from memory integration enabling the AI to reference information from previous sessions without users repeating themselves. When a user returns after days or weeks, long-term memories surface relevant facts, preferences, and goals from past conversations. The AI can reference earlier learning objectives, build on previously explained concepts, and avoid repeating content the user already knows. Multi-session learning journeys become coherent narratives rather than disconnected exchanges.

Personalization adapts AI behavior to individual user preferences extracted from memories and profiles. Communication style adjusts based on preferred formality, length, and tone stored in UserProfile preferences. Example frequency matches user preferences for few, balanced, or many examples. Questioning frequency aligns with user tolerance for Socratic method versus direct explanation. Technical depth scales to user expertise levels inferred from skill memories and past interactions. These adaptations create tailored educational experiences matching each learner's needs.

Memory feedback loops improve over time as the system learns which memories are most useful. High-relevance memories retrieved frequently develop strong importance scores through access frequency and reinforcement. Low-utility memories decay naturally as they remain unused. User corrections captured in conversations create new memories contradicting old ones enabling belief updates. The memory system evolves alongside user knowledge and preferences producing increasingly accurate personalization over extended relationships.

## Performance Characteristics

The memory system delivers sub-100ms retrieval latency under typical loads supporting real-time conversational interactions. Short-term memory retrieval from Redis cache completes in under 5ms for cache hits with single-digit millisecond round trips. Working memory retrieval including MongoDB queries for recent messages completes in 10-20ms with properly indexed collections. Long-term memory retrieval including ChromaDB semantic search, MongoDB fetching, and relevance scoring completes in 50-80ms dominating total retrieval time. User profile loading adds 5-10ms cached or 20-30ms uncached. The complete multi-tiered retrieval pipeline typically finishes within 80-100ms enabling responsive conversations.

Token reduction through summarization and selective memory retrieval achieves 60-80% savings compared to including full conversation history in prompts. Without memory management, a 50-message conversation consuming 3000+ tokens for context becomes unmaintainable within model limits. With summarization, the same conversation uses 600-800 tokens with older messages compressed into 200-token summaries while preserving key information. Long-term memory contributes only 400 tokens of highly relevant facts despite containing hundreds of memories. The system maintains conversation quality while reducing costs and staying within context windows.

Storage efficiency benefits from deduplication, compression, and archival policies. Deduplication prevents redundant storage of repeated information consolidating similar memories through merging. Archived memories are excluded from active queries reducing search space and improving query performance. Vector embeddings at 384 dimensions using BGE-small are 3-4x smaller than alternatives like OpenAI embeddings at 1536 dimensions reducing ChromaDB storage and search costs. The user-specific collection sharding strategy enables selective deletion and independent scaling.

Scalability supports millions of users through distributed architecture and efficient algorithms. Redis caching prevents database hotspots by absorbing read traffic with 75-80% hit rates. MongoDB sharding distributes memory entries across servers using userId shard keys enabling horizontal scaling. ChromaDB user-specific collections shard naturally across instances without complex partitioning. Background jobs process users in configurable batches preventing resource exhaustion during maintenance. The architecture scales linearly with user growth by adding cache, database, and vector store capacity independently.

