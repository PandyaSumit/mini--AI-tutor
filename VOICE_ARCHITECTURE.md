# Voice AI Tutor Architecture Documentation

## Table of Contents
1. [Overview](#overview)
2. [System Architecture](#system-architecture)
3. [Speech-to-Text (STT) Implementation](#speech-to-text-stt-implementation)
4. [Text-to-Speech (TTS) Implementation](#text-to-speech-tts-implementation)
5. [WebSocket Communication Flow](#websocket-communication-flow)
6. [Complete End-to-End Flow](#complete-end-to-end-flow)
7. [Component Details](#component-details)
8. [Error Handling & Fallbacks](#error-handling--fallbacks)
9. [Data Models](#data-models)
10. [Deployment & Testing](#deployment--testing)

---

## Overview

The Voice AI Tutor is a **real-time, two-way voice communication system** that enables users to have voice conversations with an AI tutor. The system uses a **hybrid approach** combining server-side and client-side speech processing with intelligent fallback mechanisms.

### Key Features
- **100% Free Operation**: Uses browser Web Speech API (no API costs)
- **Real-time Communication**: WebSocket-based bidirectional streaming
- **Intelligent Fallbacks**: Multiple STT options with automatic switching
- **Low Latency**: Client-side TTS for instant responses
- **Conversation Persistence**: All interactions saved to MongoDB
- **User Controls**: Stop, pause, and toggle voice output

### Technology Stack
- **Frontend**: React, Web Speech API (SpeechRecognition + SpeechSynthesis)
- **Backend**: Node.js, Socket.IO, Express
- **AI/LLM**: Groq (LLaMA models)
- **Database**: MongoDB with Mongoose
- **Authentication**: JWT tokens

---

## System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         FRONTEND (React)                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  VoiceChat.jsx   ‚îÇ    ‚îÇ  browserSTT.js   ‚îÇ    ‚îÇ  ttsService.js‚îÇ ‚îÇ
‚îÇ  ‚îÇ  (UI Component)  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚î§ (Speech Input)   ‚îÇ    ‚îÇ (Speech Output)‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ           ‚îÇ                       ‚îÇ                       ‚îÇ          ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                                   ‚îÇ                                  ‚îÇ
‚îÇ                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ
‚îÇ                          ‚îÇvoiceWebSocket.js ‚îÇ                        ‚îÇ
‚îÇ                          ‚îÇ (Socket Manager) ‚îÇ                        ‚îÇ
‚îÇ                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                            WebSocket (Socket.IO)
                                    ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         BACKEND (Node.js)                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇvoiceWebSocket.js ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ voiceHandlers.js ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇvoiceOrchestrator‚îÇ ‚îÇ
‚îÇ  ‚îÇ(Socket Endpoint) ‚îÇ    ‚îÇ (Event Handlers) ‚îÇ    ‚îÇ   (Business    ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ     Logic)     ‚îÇ ‚îÇ
‚îÇ                                   ‚îÇ               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                   ‚îÇ                       ‚îÇ          ‚îÇ
‚îÇ                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ                          ‚îÇ VoiceSession.js  ‚îÇ    ‚îÇaiOrchestrator ‚îÇ ‚îÇ
‚îÇ                          ‚îÇ   (Model)        ‚îÇ    ‚îÇ  (AI Service) ‚îÇ ‚îÇ
‚îÇ                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                           ‚îÇ          ‚îÇ
‚îÇ                                                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ                                                   ‚îÇ  Groq LLM API ‚îÇ ‚îÇ
‚îÇ                                                   ‚îÇ  (External)   ‚îÇ ‚îÇ
‚îÇ                                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚îÇ    MongoDB     ‚îÇ
                            ‚îÇ   (Database)   ‚îÇ
                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Architecture Layers

1. **Presentation Layer**: React components (VoiceChat.jsx)
2. **Client Services Layer**: browserSTT, ttsService, voiceWebSocket
3. **Communication Layer**: Socket.IO WebSocket connection
4. **API Layer**: Express + Socket.IO server endpoints
5. **Business Logic Layer**: voiceOrchestrator, aiOrchestrator
6. **Data Layer**: MongoDB with Mongoose models
7. **External Services**: Groq LLM API

---

## Speech-to-Text (STT) Implementation

### STT Architecture Overview

The system implements a **three-tier fallback mechanism** for speech recognition:

```
User Speaks
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Primary: Hugging Face   ‚îÇ (Server-side, high quality)
‚îÇ - Whisper-based model   ‚îÇ
‚îÇ - Requires API key      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ (if fails/not configured)
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Fallback 1: OpenAI      ‚îÇ (Server-side, premium)
‚îÇ - Whisper API           ‚îÇ
‚îÇ - Requires API key      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ (if fails/not configured)
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Fallback 2: Browser API ‚îÇ (Client-side, FREE)
‚îÇ - Web Speech API        ‚îÇ
‚îÇ - No API key needed     ‚îÇ
‚îÇ - Chrome/Edge/Safari    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Browser STT Implementation

**File**: `frontend/src/services/browserSTT.js`

#### Class Structure

```javascript
class BrowserSTT {
  constructor() {
    this.recognition = null;           // SpeechRecognition instance
    this.isListening = false;          // Listening state
    this.transcript = '';              // Final transcript accumulator
    this.interimTranscript = '';       // Interim results buffer
    this.onTranscriptCallback = null;  // Callback for results
    this.onErrorCallback = null;       // Callback for errors
  }
}
```

#### Key Methods

##### 1. `isSupported()`
Checks if the browser supports Web Speech API:
```javascript
isSupported() {
  return 'webkitSpeechRecognition' in window || 'SpeechRecognition' in window;
}
```

##### 2. `start(language = 'en-US')`
Initializes and starts speech recognition:
```javascript
start(language = 'en-US') {
  // Create SpeechRecognition instance
  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
  this.recognition = new SpeechRecognition();

  // Configure recognition
  this.recognition.continuous = true;        // Keep listening
  this.recognition.interimResults = true;    // Get interim results
  this.recognition.lang = language;          // Set language
  this.recognition.maxAlternatives = 1;      // Single best result

  // Event handlers
  this.recognition.onresult = (event) => { /* ... */ };
  this.recognition.onerror = (event) => { /* ... */ };
  this.recognition.onend = () => { /* ... */ };

  // Start listening
  this.recognition.start();
  this.isListening = true;
}
```

##### 3. `onresult` Event Handler
Processes speech recognition results:
```javascript
this.recognition.onresult = (event) => {
  let interim = '';

  // Process all results
  for (let i = event.resultIndex; i < event.results.length; i++) {
    const result = event.results[i];
    const transcriptPart = result[0].transcript;

    if (result.isFinal) {
      // Final result - append to transcript
      this.transcript += transcriptPart + ' ';
      console.log('üé§ Final:', transcriptPart);
    } else {
      // Interim result - store temporarily
      interim += transcriptPart;
      console.log('üé§ Interim:', transcriptPart);
    }
  }

  // Update interim transcript
  this.interimTranscript = interim;

  // Notify callback with current transcript
  if (this.onTranscriptCallback) {
    this.onTranscriptCallback({
      transcript: this.transcript,
      interim: interim,
      isFinal: false
    });
  }
};
```

##### 4. `stop()`
Stops recognition and returns final transcript:
```javascript
stop() {
  if (this.recognition && this.isListening) {
    this.recognition.stop();
  }

  // Combine final + pending interim results
  const finalTranscript = this.transcript + (this.interimTranscript || '');
  console.log('üé§ Browser STT stop() returning:', finalTranscript);

  // Reset state
  this.isListening = false;
  this.transcript = '';
  this.interimTranscript = '';

  return finalTranscript.trim();
}
```

### Server-Side STT Flow

**File**: `backend/socketHandlers/voiceHandlers.js`

#### Voice Recording Handler
```javascript
socket.on('voice:audio-chunk', async (data) => {
  const { sessionId, audioChunk, isLastChunk } = data;

  // Store audio chunk
  session.audioBuffer.push(audioChunk);

  if (isLastChunk) {
    // All chunks received, process audio
    const completeAudio = Buffer.concat(session.audioBuffer);

    try {
      // Try Hugging Face STT
      transcript = await huggingFaceSTT(completeAudio);
    } catch (error) {
      try {
        // Fallback to OpenAI Whisper
        transcript = await openAIWhisper(completeAudio);
      } catch (error) {
        // Suggest browser STT fallback
        socket.emit('voice:use-browser-stt', {
          message: 'Using browser speech recognition (100% FREE, instant!)'
        });
        return;
      }
    }

    // Emit transcript to client
    socket.emit('voice:transcribed', { text: transcript });
  }
});
```

#### Browser STT Text Handler
```javascript
socket.on('voice:text-message', async (data) => {
  const { sessionId, text } = data;

  // Find or create conversation
  let conversation = await Conversation.findById(session.conversationId);

  if (!conversation) {
    conversation = new Conversation({
      user: session.userId,
      title: `Voice Chat - ${new Date().toLocaleString()}`,
      metadata: {
        isVoiceSession: true,
        sessionId: session._id,
        language: session.settings.language
      }
    });
    await conversation.save();

    session.conversationId = conversation._id;
    await session.save();
  }

  // Process text as if it were transcribed speech
  const response = await voiceOrchestrator.generateAIResponse(
    conversation._id,
    session.userId,
    text
  );

  // Send AI response
  socket.emit('voice:response', {
    text: response.text,
    shouldSpeak: session.settings.ttsEnabled
  });
});
```

---

## Text-to-Speech (TTS) Implementation

### TTS Architecture Overview

```
AI Response Generated
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Backend sends response ‚îÇ
‚îÇ  with shouldSpeak flag  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº (WebSocket event)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   voiceWebSocket.js     ‚îÇ
‚îÇ   receives response     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº (if shouldSpeak)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ttsService.speak()    ‚îÇ
‚îÇ   Browser API called    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ Emit 'tts-started' ‚îÄ‚îÄ‚ñ∫ UI shows "Speaking..."
         ‚îÇ
         ‚ñº (Browser speaks)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ SpeechSynthesis speaks  ‚îÇ
‚îÇ   text out loud         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ Emit 'tts-ended' ‚îÄ‚îÄ‚ñ∫ UI hides indicator
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ notifyTTSComplete()     ‚îÇ
‚îÇ Server ready for next   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### TTS Service Implementation

**File**: `frontend/src/services/ttsService.js`

#### Class Structure

```javascript
class TTSService {
  constructor() {
    this.synthesis = window.speechSynthesis;
    this.currentUtterance = null;
    this.isSpeaking = false;
    this.isPaused = false;
    this.defaultSettings = {
      rate: 1.0,      // Speed (0.1 to 10)
      pitch: 1.0,     // Pitch (0 to 2)
      volume: 1.0,    // Volume (0 to 1)
      voice: null     // Voice object
    };
  }
}
```

#### Key Methods

##### 1. `speak(text, settings = {})`
Main method for speaking text:

```javascript
async speak(text, settings = {}) {
  return new Promise(async (resolve, reject) => {
    // Validation: Empty text check
    if (!text || text.trim().length === 0) {
      console.warn('‚ö†Ô∏è No text to speak');
      resolve({ success: false, message: 'No text to speak' });
      return;
    }

    // Cancel any ongoing speech
    this.stop();

    // Long text handling (>300 chars)
    if (text.length > 300) {
      console.log('üîä Speaking long text in chunks...');
      try {
        await this.speakLongText(text, settings);
        resolve({ success: true });
      } catch (error) {
        reject(error);
      }
      return;
    }

    // Create utterance
    this.currentUtterance = new SpeechSynthesisUtterance(text);

    // Apply settings
    const finalSettings = { ...this.defaultSettings, ...settings };
    this.currentUtterance.rate = finalSettings.rate;
    this.currentUtterance.pitch = finalSettings.pitch;
    this.currentUtterance.volume = finalSettings.volume;
    if (finalSettings.voice) {
      this.currentUtterance.voice = finalSettings.voice;
    }

    // Event handlers
    this.currentUtterance.onstart = () => {
      this.isSpeaking = true;
      console.log('üîä TTS started:', text.substring(0, 50) + '...');
    };

    this.currentUtterance.onend = () => {
      this.isSpeaking = false;
      this.isPaused = false;
      console.log('‚úÖ TTS completed');
      resolve({ success: true });
    };

    this.currentUtterance.onerror = (error) => {
      this.isSpeaking = false;
      this.isPaused = false;
      console.error('‚ùå TTS error:', error);

      // Don't reject on user cancellation
      if (error.error === 'canceled' || error.error === 'interrupted') {
        resolve({ success: false, message: 'TTS was canceled' });
      } else {
        reject(error);
      }
    };

    // Start speaking
    try {
      this.synthesis.speak(this.currentUtterance);
    } catch (error) {
      this.isSpeaking = false;
      console.error('‚ùå Failed to start TTS:', error);
      reject(error);
    }
  });
}
```

##### 2. `speakLongText(text, settings = {})`
Chunks long text for better TTS performance:

```javascript
async speakLongText(text, settings = {}) {
  // Split text into sentences
  const sentences = text.match(/[^.!?]+[.!?]+/g) || [text];
  const chunks = [];
  let currentChunk = '';

  // Create chunks of ~300 chars
  for (const sentence of sentences) {
    if (currentChunk.length + sentence.length > 300) {
      if (currentChunk) chunks.push(currentChunk.trim());
      currentChunk = sentence;
    } else {
      currentChunk += ' ' + sentence;
    }
  }
  if (currentChunk) chunks.push(currentChunk.trim());

  // Speak each chunk sequentially
  for (let i = 0; i < chunks.length; i++) {
    console.log(`üîä Speaking chunk ${i + 1}/${chunks.length}`);
    await this.speak(chunks[i], settings);

    // Small pause between chunks
    if (i < chunks.length - 1) {
      await new Promise(resolve => setTimeout(resolve, 200));
    }
  }
}
```

##### 3. `stop()`
Stops current speech:

```javascript
stop() {
  if (this.synthesis.speaking) {
    this.synthesis.cancel();
    this.isSpeaking = false;
    this.isPaused = false;
    console.log('‚èπÔ∏è TTS stopped');
  }
}
```

##### 4. Voice Management

```javascript
// Get available voices
getVoices() {
  return this.synthesis.getVoices();
}

// Set voice by name
setVoice(voiceName) {
  const voices = this.getVoices();
  const voice = voices.find(v => v.name === voiceName);
  if (voice) {
    this.defaultSettings.voice = voice;
    return true;
  }
  return false;
}
```

### WebSocket TTS Integration

**File**: `frontend/src/services/voiceWebSocket.js`

```javascript
this.socket.on('voice:response', async (data) => {
  console.log('ü§ñ AI Response:', data.text);
  this.emit('response', data);

  // Automatically speak if TTS is enabled
  if (data.shouldSpeak) {
    console.log('üîä Starting TTS for AI response...');
    this.emit('tts-started'); // Notify UI

    try {
      const result = await ttsService.speak(data.text);
      console.log('‚úÖ TTS completed:', result);
      this.emit('tts-ended'); // Notify UI
      this.notifyTTSComplete(); // Notify server
    } catch (error) {
      console.error('‚ùå TTS error:', error);
      this.emit('tts-ended'); // Notify UI even on error
      this.notifyTTSComplete(); // Notify server
    }
  } else {
    console.log('üîá TTS disabled, skipping speech');
    this.notifyTTSComplete(); // Notify server immediately
  }
});
```

### UI Integration

**File**: `frontend/src/components/VoiceChat.jsx`

#### TTS State Management

```javascript
const [isSpeaking, setIsSpeaking] = useState(false);
const [ttsEnabled, setTtsEnabled] = useState(true);

// Listen to TTS lifecycle events
voiceWebSocket.on('tts-started', () => {
  console.log('üîä TTS started - showing indicator');
  setIsSpeaking(true);
});

voiceWebSocket.on('tts-ended', () => {
  console.log('üîä TTS ended - hiding indicator');
  setIsSpeaking(false);
});
```

#### TTS Controls

```javascript
// Toggle TTS on/off
const toggleTTS = () => {
  setTtsEnabled(!ttsEnabled);
};

// Stop speaking immediately
const stopSpeaking = () => {
  console.log('‚èπÔ∏è User stopped TTS');
  ttsService.stop();
  setIsSpeaking(false);
  voiceWebSocket.notifyTTSComplete();
};
```

#### UI Elements

```jsx
{/* TTS Toggle Button */}
<button
  onClick={toggleTTS}
  className="p-2 hover:bg-gray-100 dark:hover:bg-gray-700 rounded-lg transition-colors"
  title={ttsEnabled ? "Disable voice output" : "Enable voice output"}
>
  {ttsEnabled ? <Volume2 size={20} /> : <VolumeX size={20} />}
</button>

{/* Speaking Indicator with Stop Button */}
{isSpeaking && (
  <div className="bg-blue-50 dark:bg-blue-900/20 rounded-lg p-3 flex items-center gap-3">
    <Volume2 className="animate-pulse text-blue-600" size={16} />
    <span className="text-sm text-blue-600 dark:text-blue-400">Speaking...</span>
    <button
      onClick={stopSpeaking}
      className="ml-auto p-1 hover:bg-blue-100 dark:hover:bg-blue-800 rounded transition-colors"
    >
      <StopCircle size={16} className="text-blue-600 dark:text-blue-400" />
    </button>
  </div>
)}
```

---

## WebSocket Communication Flow

### Connection Establishment

```
Frontend                          Backend
   ‚îÇ                                 ‚îÇ
   ‚îÇ  1. Connect with JWT token      ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
   ‚îÇ                                 ‚îÇ 2. Verify token
   ‚îÇ                                 ‚îÇ    Extract userId
   ‚îÇ  3. connection event            ‚îÇ
   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ                                 ‚îÇ
   ‚îÇ  4. voice:join                  ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
   ‚îÇ                                 ‚îÇ 5. Create/find session
   ‚îÇ                                 ‚îÇ    Initialize state
   ‚îÇ  6. voice:session-started       ‚îÇ
   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ                                 ‚îÇ
```

### Voice Interaction Flow

```
Frontend                          Backend
   ‚îÇ                                 ‚îÇ
   ‚îÇ  User starts speaking           ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
   ‚îÇ BrowserSTT  ‚îÇ                   ‚îÇ
   ‚îÇ  listening  ‚îÇ                   ‚îÇ
   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
   ‚îÇ                                 ‚îÇ
   ‚îÇ  User stops speaking            ‚îÇ
   ‚îÇ  voice:text-message             ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
   ‚îÇ                                 ‚îÇ Find/create conversation
   ‚îÇ                                 ‚îÇ Save user message
   ‚îÇ                                 ‚îÇ Call AI orchestrator
   ‚îÇ                                 ‚îÇ Generate AI response
   ‚îÇ                                 ‚îÇ Save AI message
   ‚îÇ  voice:response                 ‚îÇ
   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ                                 ‚îÇ
   ‚îÇ  tts-started event              ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
   ‚îÇ TTS Service ‚îÇ                   ‚îÇ
   ‚îÇ  speaking   ‚îÇ                   ‚îÇ
   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
   ‚îÇ                                 ‚îÇ
   ‚îÇ  tts-ended event                ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
   ‚îÇ             ‚îÇ                   ‚îÇ
   ‚îÇ  voice:tts-complete             ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
   ‚îÇ                                 ‚îÇ Mark session ready
   ‚îÇ  voice:ready                    ‚îÇ
   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ                                 ‚îÇ
```

### Event Types

#### Client ‚Üí Server Events

| Event Name | Payload | Purpose |
|------------|---------|---------|
| `voice:join` | `{ language }` | Join/create voice session |
| `voice:leave` | `{ sessionId }` | Leave voice session |
| `voice:audio-chunk` | `{ sessionId, audioChunk, isLastChunk }` | Send recorded audio |
| `voice:text-message` | `{ sessionId, text }` | Send browser STT transcript |
| `voice:tts-complete` | `{ sessionId }` | Notify TTS finished |
| `voice:settings-update` | `{ sessionId, settings }` | Update session settings |

#### Server ‚Üí Client Events

| Event Name | Payload | Purpose |
|------------|---------|---------|
| `voice:session-started` | `{ sessionId, settings }` | Session initialized |
| `voice:use-browser-stt` | `{ message }` | Switch to browser STT |
| `voice:transcribed` | `{ text }` | Server STT result |
| `voice:processing` | `{ status }` | AI processing update |
| `voice:response` | `{ text, shouldSpeak }` | AI response ready |
| `voice:ready` | `{}` | Ready for next input |
| `voice:error` | `{ error }` | Error occurred |

---

## Complete End-to-End Flow

### Scenario: User asks "What is machine learning?"

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 1: USER SPEAKS                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Frontend (VoiceChat.jsx)
  ‚îÇ
  ‚îú‚îÄ User clicks microphone button
  ‚îú‚îÄ handleVoiceInput() called
  ‚îú‚îÄ setIsRecording(true)
  ‚îÇ
  ‚îî‚îÄ‚ñ∫ browserSTT.start('en-US')
        ‚îÇ
        ‚îú‚îÄ SpeechRecognition starts listening
        ‚îú‚îÄ onresult: "What" (interim)
        ‚îú‚îÄ onresult: "What is" (interim)
        ‚îú‚îÄ onresult: "What is machine" (interim)
        ‚îú‚îÄ onresult: "What is machine learning" (final)
        ‚îÇ
        ‚îî‚îÄ‚ñ∫ User clicks stop button
              ‚îÇ
              ‚îî‚îÄ‚ñ∫ browserSTT.stop()
                    ‚îú‚îÄ Returns: "What is machine learning?"
                    ‚îú‚îÄ setTranscript("What is machine learning?")
                    ‚îî‚îÄ‚ñ∫ sendTextMessage()

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 2: SEND TO BACKEND                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Frontend (voiceWebSocket.js)
  ‚îÇ
  ‚îú‚îÄ sendTextMessage("What is machine learning?")
  ‚îÇ
  ‚îî‚îÄ‚ñ∫ socket.emit('voice:text-message', {
        sessionId: currentSessionId,
        text: "What is machine learning?"
      })

        ‚îÇ
        ‚îÇ WebSocket
        ‚ñº

Backend (voiceHandlers.js)
  ‚îÇ
  ‚îú‚îÄ Event: 'voice:text-message' received
  ‚îú‚îÄ Extract sessionId and text
  ‚îú‚îÄ Find VoiceSession
  ‚îÇ
  ‚îú‚îÄ Find or create Conversation
  ‚îÇ   ‚îú‚îÄ Check session.conversationId
  ‚îÇ   ‚îî‚îÄ Create new Conversation if needed
  ‚îÇ       ‚îî‚îÄ conversation = new Conversation({
  ‚îÇ             user: session.userId,
  ‚îÇ             title: "Voice Chat - 11/16/2025",
  ‚îÇ             metadata: { isVoiceSession: true, ... }
  ‚îÇ           })
  ‚îÇ
  ‚îú‚îÄ Save user message
  ‚îÇ   ‚îî‚îÄ userMessage = new Message({
  ‚îÇ         conversation: conversation._id,
  ‚îÇ         user: session.userId,
  ‚îÇ         role: 'user',
  ‚îÇ         content: "What is machine learning?",
  ‚îÇ         metadata: { isVoice: false }
  ‚îÇ       })
  ‚îÇ
  ‚îî‚îÄ‚ñ∫ voiceOrchestrator.generateAIResponse(...)

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 3: AI PROCESSING                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Backend (voiceOrchestrator.js)
  ‚îÇ
  ‚îú‚îÄ Emit 'voice:processing' to client
  ‚îÇ   ‚îî‚îÄ‚ñ∫ socket.emit('voice:processing', { status: 'Generating response...' })
  ‚îÇ
  ‚îú‚îÄ Build conversation context
  ‚îÇ   ‚îú‚îÄ Fetch last 10 messages from conversation
  ‚îÇ   ‚îî‚îÄ Format as [{role: 'user', content: '...'}, ...]
  ‚îÇ
  ‚îî‚îÄ‚ñ∫ aiOrchestrator.tutorChat(conversationId, userId, message, context)
        ‚îÇ
        ‚îî‚îÄ‚ñ∫ Groq API call
              ‚îú‚îÄ Model: llama-3.1-70b-versatile
              ‚îú‚îÄ Messages: conversation context
              ‚îÇ
              ‚îî‚îÄ‚ñ∫ Returns: "Machine learning is a subset of artificial
                           intelligence that enables computers to learn
                           from data without being explicitly programmed..."

Backend (voiceOrchestrator.js)
  ‚îÇ
  ‚îú‚îÄ Receive AI response
  ‚îú‚îÄ Save AI message to database
  ‚îÇ   ‚îî‚îÄ aiMessage = new Message({
  ‚îÇ         conversation: conversation._id,
  ‚îÇ         role: 'assistant',
  ‚îÇ         content: "Machine learning is...",
  ‚îÇ         metadata: { model: 'groq-llama', tokens: 145 }
  ‚îÇ       })
  ‚îÇ
  ‚îî‚îÄ‚ñ∫ socket.emit('voice:response', {
        text: "Machine learning is...",
        shouldSpeak: session.settings.ttsEnabled  // true
      })

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 4: RECEIVE & DISPLAY RESPONSE                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Frontend (voiceWebSocket.js)
  ‚îÇ
  ‚îú‚îÄ Event: 'voice:response' received
  ‚îú‚îÄ console.log('ü§ñ AI Response:', data.text)
  ‚îú‚îÄ this.emit('response', data)
  ‚îÇ   ‚îî‚îÄ‚ñ∫ VoiceChat.jsx receives and displays message
  ‚îÇ
  ‚îî‚îÄ if (data.shouldSpeak) {
       ‚îÇ
       ‚îú‚îÄ this.emit('tts-started')
       ‚îÇ   ‚îî‚îÄ‚ñ∫ VoiceChat: setIsSpeaking(true)
       ‚îÇ       ‚îî‚îÄ‚ñ∫ UI shows "Speaking..." indicator
       ‚îÇ
       ‚îî‚îÄ‚ñ∫ ttsService.speak("Machine learning is...")
             ‚îÇ
             ‚îú‚îÄ Create SpeechSynthesisUtterance
             ‚îú‚îÄ Set rate: 1.0, pitch: 1.0, volume: 1.0
             ‚îú‚îÄ synthesis.speak(utterance)
             ‚îÇ
             ‚îú‚îÄ onstart: console.log('üîä TTS started')
             ‚îú‚îÄ Browser speaks text aloud (5-10 seconds)
             ‚îÇ
             ‚îî‚îÄ onend: console.log('‚úÖ TTS completed')
     }

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 5: TTS COMPLETION                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Frontend (voiceWebSocket.js)
  ‚îÇ
  ‚îú‚îÄ TTS completed successfully
  ‚îú‚îÄ this.emit('tts-ended')
  ‚îÇ   ‚îî‚îÄ‚ñ∫ VoiceChat: setIsSpeaking(false)
  ‚îÇ       ‚îî‚îÄ‚ñ∫ UI hides "Speaking..." indicator
  ‚îÇ
  ‚îî‚îÄ‚ñ∫ this.notifyTTSComplete()
        ‚îÇ
        ‚îî‚îÄ‚ñ∫ socket.emit('voice:tts-complete', { sessionId })

              ‚îÇ
              ‚îÇ WebSocket
              ‚ñº

Backend (voiceHandlers.js)
  ‚îÇ
  ‚îú‚îÄ Event: 'voice:tts-complete' received
  ‚îú‚îÄ Mark session as ready
  ‚îÇ   ‚îî‚îÄ session.isProcessing = false
  ‚îÇ
  ‚îî‚îÄ‚ñ∫ socket.emit('voice:ready', {})

Frontend (VoiceChat.jsx)
  ‚îÇ
  ‚îú‚îÄ Event: 'voice:ready' received
  ‚îú‚îÄ setIsProcessing(false)
  ‚îú‚îÄ setIsSpeaking(false)
  ‚îÇ
  ‚îî‚îÄ‚ñ∫ System ready for next user input
```

### Complete Flow Diagram

```
User ‚Üí Microphone ‚Üí Browser STT ‚Üí WebSocket ‚Üí Backend
                                                  ‚îú‚îÄ Create/Find Session
                                                  ‚îú‚îÄ Create/Find Conversation
                                                  ‚îú‚îÄ Save User Message
                                                  ‚îú‚îÄ AI Orchestrator
                                                  ‚îÇ   ‚îî‚îÄ Groq LLM
                                                  ‚îú‚îÄ Save AI Message
                                                  ‚îî‚îÄ Emit Response

Backend ‚Üí WebSocket ‚Üí voiceWebSocket ‚Üí TTS Service ‚Üí Speaker ‚Üí User
                                    ‚îî‚îÄ UI Updates
```

---

## Component Details

### Frontend Components

#### 1. VoiceChat.jsx
**Purpose**: Main UI component for voice chat interface

**Key Responsibilities**:
- Manage voice session lifecycle
- Handle user interactions (mic button, stop button, TTS toggle)
- Display messages and status indicators
- Coordinate between browserSTT, ttsService, and voiceWebSocket

**State Variables**:
```javascript
const [isConnected, setIsConnected] = useState(false);       // WebSocket connection
const [isRecording, setIsRecording] = useState(false);       // Microphone active
const [isProcessing, setIsProcessing] = useState(false);     // AI processing
const [isSpeaking, setIsSpeaking] = useState(false);         // TTS active
const [transcript, setTranscript] = useState('');            // Current transcript
const [messages, setMessages] = useState([]);                // Chat messages
const [error, setError] = useState(null);                    // Error messages
const [infoMessage, setInfoMessage] = useState(null);        // Info messages
const [ttsEnabled, setTtsEnabled] = useState(true);          // TTS on/off
const [useBrowserSTT, setUseBrowserSTT] = useState(false);   // STT mode
```

**Key Functions**:
```javascript
// Initialize WebSocket connection
const initializeVoiceSession = async () => { /* ... */ }

// Start voice recording
const handleVoiceInput = () => { /* ... */ }

// Stop recording and send transcript
const stopRecording = () => { /* ... */ }

// Send text message (browser STT)
const sendTextMessage = () => { /* ... */ }

// Stop TTS playback
const stopSpeaking = () => { /* ... */ }

// Toggle TTS on/off
const toggleTTS = () => { /* ... */ }
```

#### 2. browserSTT.js
**Purpose**: Browser-based speech recognition service

**Key Features**:
- Continuous listening with interim results
- Language support
- Error handling
- Transcript accumulation

**Configuration**:
```javascript
continuous: true          // Keep listening until stopped
interimResults: true      // Get real-time interim results
maxAlternatives: 1        // Single best result
lang: 'en-US'            // Language code
```

#### 3. ttsService.js
**Purpose**: Browser-based text-to-speech service

**Key Features**:
- Speech synthesis with customization
- Long text chunking
- Voice selection
- Playback controls (pause, resume, stop)

**Customization Options**:
```javascript
rate: 0.1 - 10.0    // Speech speed
pitch: 0 - 2.0      // Voice pitch
volume: 0 - 1.0     // Volume level
voice: Voice        // Selected voice
```

#### 4. voiceWebSocket.js
**Purpose**: WebSocket connection manager

**Key Features**:
- Socket.IO integration
- Event emission and listening
- TTS lifecycle management
- Auto-reconnection

**Methods**:
```javascript
connect()                              // Establish connection
disconnect()                           // Close connection
joinSession(language)                  // Join voice session
leaveSession(sessionId)                // Leave session
sendAudioChunk(sessionId, chunk, last) // Send audio data
sendTextMessage(sessionId, text)       // Send text
notifyTTSComplete(sessionId)           // Notify TTS done
updateSettings(sessionId, settings)    // Update settings
```

### Backend Components

#### 1. voiceWebSocket.js (Backend)
**Purpose**: Socket.IO server configuration

**Middleware**:
```javascript
// JWT authentication
io.use(async (socket, next) => {
  const token = socket.handshake.auth.token;
  if (!token) return next(new Error('Authentication required'));

  const decoded = jwt.verify(token, process.env.JWT_SECRET);
  socket.userId = decoded.userId;
  next();
});
```

**Connection Handling**:
```javascript
io.on('connection', (socket) => {
  console.log('User connected:', socket.userId);

  // Register event handlers
  registerVoiceHandlers(io);

  socket.on('disconnect', () => {
    console.log('User disconnected:', socket.userId);
  });
});
```

#### 2. voiceHandlers.js
**Purpose**: Socket event handlers

**Event Handlers**:
```javascript
// Join session
socket.on('voice:join', async (data) => { /* ... */ })

// Leave session
socket.on('voice:leave', async (data) => { /* ... */ })

// Audio chunk (server STT)
socket.on('voice:audio-chunk', async (data) => { /* ... */ })

// Text message (browser STT)
socket.on('voice:text-message', async (data) => { /* ... */ })

// TTS complete
socket.on('voice:tts-complete', async (data) => { /* ... */ })

// Settings update
socket.on('voice:settings-update', async (data) => { /* ... */ })
```

#### 3. voiceOrchestrator.js
**Purpose**: Business logic for voice sessions

**Key Methods**:
```javascript
// Generate AI response
async generateAIResponse(conversationId, userId, userMessage) {
  // 1. Emit processing status
  // 2. Fetch conversation context
  // 3. Call AI orchestrator
  // 4. Save AI message
  // 5. Return response
}

// Process audio with STT
async processAudioWithSTT(audioBuffer) {
  // 1. Try Hugging Face
  // 2. Fallback to OpenAI Whisper
  // 3. Fallback to browser STT
}
```

#### 4. aiOrchestrator.js
**Purpose**: AI/LLM integration

**Key Method**:
```javascript
async tutorChat(conversationId, userId, message, context) {
  const response = await groq.chat.completions.create({
    model: 'llama-3.1-70b-versatile',
    messages: [
      { role: 'system', content: 'You are a helpful AI tutor...' },
      ...context,
      { role: 'user', content: message }
    ],
    temperature: 0.7,
    max_tokens: 1000
  });

  return {
    text: response.choices[0].message.content,
    usage: response.usage
  };
}
```

---

## Error Handling & Fallbacks

### STT Error Handling

#### Fallback Chain
```javascript
async function processAudio(audioBuffer) {
  try {
    // Primary: Hugging Face
    return await huggingFaceSTT(audioBuffer);
  } catch (error) {
    console.warn('Hugging Face STT failed:', error.message);

    try {
      // Fallback 1: OpenAI Whisper
      return await openAIWhisper(audioBuffer);
    } catch (error) {
      console.warn('OpenAI Whisper failed:', error.message);

      // Fallback 2: Browser STT
      socket.emit('voice:use-browser-stt', {
        message: 'Using browser speech recognition (100% FREE, instant!)'
      });
      return null;
    }
  }
}
```

#### Browser STT Error Handling
```javascript
this.recognition.onerror = (event) => {
  console.error('üé§ Browser STT error:', event.error);

  const errorMessages = {
    'no-speech': 'No speech detected. Please try again.',
    'audio-capture': 'Microphone not accessible. Check permissions.',
    'not-allowed': 'Microphone permission denied.',
    'network': 'Network error. Please check your connection.',
    'aborted': 'Speech recognition aborted.'
  };

  const message = errorMessages[event.error] || `Error: ${event.error}`;

  if (this.onErrorCallback) {
    this.onErrorCallback(message);
  }
};
```

### TTS Error Handling

#### Graceful Degradation
```javascript
this.currentUtterance.onerror = (error) => {
  console.error('‚ùå TTS error:', error);

  // Don't throw error on user cancellation
  if (error.error === 'canceled' || error.error === 'interrupted') {
    resolve({ success: false, message: 'TTS was canceled' });
  } else {
    // Log error but continue flow
    reject(error);
  }
};
```

#### Always Notify Server
```javascript
try {
  await ttsService.speak(data.text);
  this.emit('tts-ended');
  this.notifyTTSComplete(); // ‚úÖ Notify on success
} catch (error) {
  console.error('‚ùå TTS error:', error);
  this.emit('tts-ended');
  this.notifyTTSComplete(); // ‚úÖ Notify on error too
}
```

### WebSocket Error Handling

#### Connection Errors
```javascript
voiceWebSocket.on('error', (data) => {
  console.error('‚ùå Voice error:', data.error);
  setError(data.error);
  setIsProcessing(false);
  setIsRecording(false);
});
```

#### Auto-Reconnection
```javascript
this.socket.on('disconnect', () => {
  console.log('üîå WebSocket disconnected');
  this.connected = false;
  this.emit('disconnected');
});

this.socket.on('connect', () => {
  console.log('‚úÖ WebSocket reconnected');
  this.connected = true;
  this.emit('connected');
});
```

### Database Error Handling

#### Conversation Creation
```javascript
try {
  conversation = new Conversation({
    user: session.userId,
    title: `Voice Chat - ${new Date().toLocaleString()}`,
    metadata: { isVoiceSession: true }
  });
  await conversation.save();
} catch (error) {
  console.error('Failed to create conversation:', error);
  socket.emit('voice:error', {
    error: 'Failed to create conversation. Please try again.'
  });
  return;
}
```

#### Message Validation
```javascript
const userMessage = new Message({
  conversation: conversation._id,
  user: session.userId,
  role: 'user',
  content: text,
  metadata: { isVoice: false }
});

try {
  await userMessage.save();
} catch (error) {
  console.error('Failed to save message:', error);
  socket.emit('voice:error', {
    error: 'Failed to save your message. Please try again.'
  });
  return;
}
```

---

## Data Models

### VoiceSession Model
**File**: `backend/models/VoiceSession.js`

```javascript
{
  userId: ObjectId,              // User reference
  conversationId: ObjectId,      // Associated conversation
  status: String,                // 'active' | 'ended' | 'paused'
  language: String,              // 'en-US', 'es-ES', etc.
  audioBuffer: [Buffer],         // Temporary audio storage
  isProcessing: Boolean,         // Currently processing
  settings: {
    sttMode: String,             // 'auto' | 'browser' | 'server'
    ttsEnabled: Boolean,         // Voice output enabled
    autoSpeak: Boolean,          // Auto-speak responses
    language: String             // Speech language
  },
  metadata: {
    totalMessages: Number,       // Message count
    totalDuration: Number,       // Session duration (ms)
    averageResponseTime: Number, // Avg AI response time
    sttProvider: String,         // 'browser' | 'huggingface' | 'openai'
  },
  startedAt: Date,
  endedAt: Date,
  lastActivityAt: Date
}
```

### Conversation Model
**File**: `backend/models/Conversation.js`

```javascript
{
  user: ObjectId,                // User reference (required)
  title: String,                 // Conversation title
  topic: String,                 // 'programming' | 'mathematics' | etc.
  tags: [String],                // Custom tags
  messageCount: Number,          // Total messages
  lastMessageAt: Date,           // Last message timestamp
  isActive: Boolean,             // Active status
  metadata: {
    model: String,               // AI model used
    totalTokens: Number,         // Total tokens used
    averageResponseTime: Number, // Avg response time
    isVoiceSession: Boolean,     // Created from voice chat
    sessionId: ObjectId,         // VoiceSession reference
    language: String             // Session language
  },
  createdAt: Date,
  updatedAt: Date
}
```

### Message Model
**File**: `backend/models/Message.js`

```javascript
{
  conversation: ObjectId,        // Conversation reference (required)
  user: ObjectId,                // User reference (required)
  role: String,                  // 'user' | 'assistant' | 'system'
  content: String,               // Message text (required)
  metadata: {
    model: String,               // AI model (for assistant messages)
    tokens: Number,              // Token count
    responseTime: Number,        // Response time (ms)
    isVoice: Boolean,            // From voice input
    sttProvider: String,         // STT provider used
    confidence: Number           // STT confidence score
  },
  createdAt: Date,
  updatedAt: Date
}
```

---

## Deployment & Testing

### Environment Variables

**Backend** (`.env`):
```bash
# Server
PORT=5000
NODE_ENV=development

# Database
MONGODB_URI=mongodb://localhost:27017/ai-tutor

# Authentication
JWT_SECRET=your-secret-key-here

# AI/LLM
GROQ_API_KEY=your-groq-api-key

# STT (Optional - for server-side fallbacks)
HUGGINGFACE_API_KEY=your-hf-api-key    # Optional
OPENAI_API_KEY=your-openai-api-key     # Optional

# CORS
FRONTEND_URL=http://localhost:3000
```

**Frontend** (`.env`):
```bash
# API
REACT_APP_API_URL=http://localhost:5000
REACT_APP_WS_URL=http://localhost:5000

# Features
REACT_APP_ENABLE_VOICE=true
```

### Testing Checklist

#### STT Testing
- [ ] Browser STT works in Chrome
- [ ] Browser STT works in Edge
- [ ] Browser STT works in Safari
- [ ] Interim results display correctly
- [ ] Final transcript captured accurately
- [ ] Multiple languages work (en-US, es-ES, etc.)
- [ ] Error messages display properly
- [ ] Microphone permissions handled
- [ ] Fallback to browser STT works

#### TTS Testing
- [ ] TTS speaks AI responses
- [ ] Speaking indicator shows/hides correctly
- [ ] Stop button interrupts TTS
- [ ] TTS toggle disables/enables voice
- [ ] Long text chunks properly
- [ ] Volume/pitch/rate controls work
- [ ] Voice selection works
- [ ] Multiple languages work
- [ ] TTS completion notifies server

#### WebSocket Testing
- [ ] Connection establishes successfully
- [ ] JWT authentication works
- [ ] Session creation works
- [ ] All events emit correctly
- [ ] Reconnection works after disconnect
- [ ] Error events handled properly

#### End-to-End Testing
- [ ] Complete voice conversation works
- [ ] Messages save to database
- [ ] Conversation persists
- [ ] Context maintained across messages
- [ ] UI updates in real-time
- [ ] No memory leaks
- [ ] Performance acceptable (<2s response time)

### Browser Compatibility

| Feature | Chrome | Edge | Safari | Firefox |
|---------|--------|------|--------|---------|
| SpeechRecognition | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes (14.1+) | ‚ùå No |
| SpeechSynthesis | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes |
| WebSocket (Socket.IO) | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes |
| MediaRecorder | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes (14.1+) | ‚úÖ Yes |

**Note**: Firefox doesn't support Web Speech API for STT. Use server-side STT for Firefox users.

### Performance Optimization

#### Frontend
- Debounce interim transcript updates
- Lazy load TTS voices
- Memoize message components
- Use virtual scrolling for long conversations
- Optimize re-renders with React.memo

#### Backend
- Index MongoDB collections (user, conversation, createdAt)
- Use lean() for read-only queries
- Implement rate limiting
- Cache AI responses for common questions
- Use connection pooling for MongoDB

#### Network
- Enable gzip compression
- Use WebSocket binary mode for audio
- Implement chunked audio transfer
- Add request/response compression

---

## Summary

This Voice AI Tutor implements a **production-ready, two-way voice communication system** with:

### Key Achievements
‚úÖ **100% Free Operation** - Browser Web Speech API, no API costs
‚úÖ **Real-time Communication** - WebSocket-based, <2s latency
‚úÖ **Intelligent Fallbacks** - 3-tier STT fallback mechanism
‚úÖ **Complete Persistence** - All conversations saved to MongoDB
‚úÖ **User-Friendly UI** - Clear indicators, controls, error messages
‚úÖ **Robust Error Handling** - Graceful degradation, auto-recovery
‚úÖ **Scalable Architecture** - Modular, maintainable, extensible

### Technology Highlights
- **Frontend**: React + Web Speech API
- **Backend**: Node.js + Socket.IO + Express
- **Database**: MongoDB + Mongoose
- **AI**: Groq LLaMA 3.1 (70B)
- **Communication**: WebSocket (Socket.IO)

### File Structure Summary

**Frontend**:
- `frontend/src/components/VoiceChat.jsx` - Main UI component
- `frontend/src/services/browserSTT.js` - Speech recognition
- `frontend/src/services/ttsService.js` - Text-to-speech
- `frontend/src/services/voiceWebSocket.js` - WebSocket manager

**Backend**:
- `backend/socketHandlers/voiceHandlers.js` - Event handlers
- `backend/services/voiceOrchestrator.js` - Business logic
- `backend/services/aiOrchestrator.js` - AI integration
- `backend/models/VoiceSession.js` - Session model
- `backend/models/Conversation.js` - Conversation model
- `backend/models/Message.js` - Message model

### Next Steps for Enhancement
1. Add support for Firefox (server-side STT)
2. Implement voice activity detection (VAD)
3. Add multi-language support selector
4. Implement conversation history playback
5. Add voice analytics dashboard
6. Implement background noise cancellation
7. Add voice biometrics for user identification

---

**Document Version**: 1.0
**Last Updated**: 2025-11-16
**Author**: AI Development Team
**Status**: Production Ready
